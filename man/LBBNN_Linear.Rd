% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Layers.R
\name{LBBNN_Linear}
\alias{LBBNN_Linear}
\title{Class to generate an LBBNN feed forward layer}
\usage{
LBBNN_Linear(
  in_features,
  out_features,
  prior_inclusion,
  standard_prior,
  density_init,
  device = "cpu"
)
}
\arguments{
\item{in_features}{number of input neurons.}

\item{out_features}{number of output neurons.}

\item{prior_inclusion}{Prior inclusion probability for each weight in the layer.}

\item{standard_prior}{prior standard deviation for weights and biases.}

\item{density_init}{A vector of size two c(lower,upper) used to initialize the inclusion parameters.}

\item{device}{The device to be used. Default is CPU.}
}
\description{
Includes function for forward pass, where one can
either use the full model, or the medium probability model (MPM).
Also contains method to initialize parameters and compute KL-divergence.
}
\examples{
l1 <- LBBNN_Linear(in_features = 10,out_features = 5,prior_inclusion = 0.25,standard_prior = 1,density_init = c(-10,10),device = 'cpu')
x <- torch::torch_rand(20,10,requires_grad = FALSE)
output <- l1(x,MPM = FALSE) #the forward pass, output has shape (20,5)
print(l1$kl_div()$item()) #compute KL-divergence after the forward pass
}
