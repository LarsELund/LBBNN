% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Layers.R
\name{LBBNN_Linear}
\alias{LBBNN_Linear}
\title{Class to generate an LBBNN feed forward layer}
\usage{
LBBNN_Linear(
  in_features,
  out_features,
  prior_inclusion,
  standard_prior,
  density_init,
  flow = FALSE,
  num_transforms = 2,
  hidden_dims = c(200, 200),
  device = "cpu",
  bias_inclusion_prob = FALSE
)
}
\arguments{
\item{in_features}{number of input neurons.}

\item{out_features}{number of output neurons.}

\item{prior_inclusion}{Prior inclusion probability for each weight in the layer.}

\item{standard_prior}{prior standard deviation for weights and biases.}

\item{density_init}{A vector of size two c(lower,upper) used to initialize the inclusion parameters.}

\item{flow}{determines whether normalizing flow should be used. TRUE or FALSE}

\item{num_transforms}{Number of transformations for the flow. Default is 2.}

\item{hidden_dims}{Dimension of the hidden layer(s) in the neural networks of the RNVP transform.}

\item{device}{The device to be used. Default is CPU.}

\item{bias_inclusion_prob}{determines whether the bias should be as associated with inclusion probabilities. TRUE or FALSE}
}
\description{
Includes function for forward pass, where one can
either use the full model, or the medium probability model (MPM).
Also contains method to initialize parameters and compute KL-divergence.
}
\examples{
l1 <- LBBNN_Linear(in_features = 10,out_features = 5,prior_inclusion = 0.25,
standard_prior = 1,density_init = c(0,1),flow = FALSE)
x <- torch::torch_rand(20,10,requires_grad = FALSE)
output <- l1(x,MPM = FALSE) #the forward pass, output has shape (20,5)
print(l1$kl_div()$item()) #compute KL-divergence after the forward pass
}
