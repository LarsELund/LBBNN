% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/LBBNN_Model.R
\name{LBBNN_Net}
\alias{LBBNN_Net}
\title{Class to generate a LBBNN network}
\usage{
LBBNN_Net(
  problem_type,
  sizes,
  prior,
  std,
  inclusion_inits,
  input_skip = FALSE,
  flow = FALSE,
  num_transforms = 2,
  dims = c(200, 200),
  device = "cpu",
  link = NULL,
  nll = NULL,
  local_expl = FALSE
)
}
\arguments{
\item{problem_type}{'binary classification', 'multiclass classification' or 'regression'.}

\item{sizes}{a vector containing the sizes of layers of the network, where the first element is the input size, and the last the output size.}

\item{prior}{a vector containing the prior inclusion probabilities for each layer in the network. Length must be ONE less than sizes.}

\item{std}{a vector containing the prior standard deviation for each layer in the network. Length must be ONE less than sizes.}

\item{inclusion_inits}{a matrix of size (2,number of weight matrices). One upper and one lower bound for each layer.}

\item{input_skip}{TRUE or FALSE}

\item{flow}{whether to use normalizing flows. TRUE or FALSE.}

\item{num_transforms}{how many transformations to use in the flow.}

\item{dims}{hidden dimension for the neural network in the RNVP transform.}

\item{device}{the device to be trained on. Can be 'cpu', 'gpu' or 'mps'. Default is cpu.}
}
\description{
Generates a LBBNN composed of feed forward layers defined by LBBNN_Linear
e.g sizes = c(20,200,200,5) generates an LBBNN with 20 input variables,
two hidden layers with 200 neurons each, and an output layer of 5 neurons.
LBBNN_net also contains functions to compute kl-divergence and the density of the entire network.
}
\examples{
layers <- c(20,200,200,5) #Two hidden layers 
alpha <- c(0.3,0.5,0.9)  # One prior inclusion probability for each weight matrix 
stds <- c(1.0,1.0,1.0)  # One prior inclusion probability for each weight matrix 
inclusion_inits <- matrix(rep(c(-10,10),3),nrow = 2,ncol = 3)
prob <- 'multiclass classification'
net <- LBBNN_Net(problem_type = prob, sizes = layers, prior = alpha,std = stds
,inclusion_inits = inclusion_inits,input_skip = FALSE,flow = FALSE,device = 'cpu')
print(net)
x <- torch::torch_rand(100,20,requires_grad = FALSE) #generate some dummy data
output <- net(x) #forward pass
net$kl_div()$item() #get KL-divergence
net$density() #get the density of the network
}
